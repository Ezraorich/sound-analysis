{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cef2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1f4b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb6785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d660d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a732629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Audio files loaded from local computer\"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #self.class_name = \n",
    "        \n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        self.audio_names = audio_names\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.audio_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #root_dir = '\"D:/COVID_COUGH_SOUNDS/healthcare_sounds_few_shot/one_shot/'\n",
    "        #train/class\n",
    "        #path =\"D:/COVID_COUGH_SOUNDS/healthcare_sounds_few_shot/one_shot/\"\n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        \n",
    "        #audio_file = os.path.join(self.root_dir,self.audio_names[idx])\n",
    "        audio_file = self.audio_names[idx]\n",
    "        class_name = self.audio_names[idx].split('\\\\')[1]\n",
    "        #image = io.imread(img_name)\n",
    "        \n",
    "\n",
    "        data_waveform, rate_of_sample = torchaudio.load(audio_file)\n",
    "        \n",
    "        sample = {'audio_file': data_waveform, 'labels': class_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bce4171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = AudioDataset(path=\"D:/COVID_COUGH_SOUNDS/healthcare_sounds_few_shot/one_shot/\",\n",
    "                                           transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81ac2da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 81280]) Allergies\n",
      "1 torch.Size([1, 80640]) Asthma\n",
      "2 torch.Size([1, 111132]) Covid\n",
      "3 torch.Size([1, 81280]) Other\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(transformed_dataset)):\n",
    "    sample = transformed_dataset[i]\n",
    "\n",
    "    print(i, sample['audio_file'].size(), sample['labels'])\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b07212d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(transformed_dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13446995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 1, 80640]) ['Asthma']\n",
      "1 torch.Size([1, 1, 80208]) ['Asthma']\n",
      "2 torch.Size([1, 1, 80640]) ['Allergies']\n",
      "3 torch.Size([1, 1, 82368]) ['Pneumonia']\n",
      "4 torch.Size([1, 1, 81280]) ['Other']\n",
      "5 torch.Size([1, 1, 81280]) ['Allergies']\n",
      "6 torch.Size([1, 1, 111132]) ['Covid']\n",
      "7 torch.Size([1, 1, 80640]) ['Other']\n",
      "8 torch.Size([1, 1, 110250]) ['Covid']\n",
      "9 torch.Size([1, 1, 80208]) ['Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['audio_file'].size(),\n",
    "          sample_batched['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af9370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
