{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1009b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One shot learning for audio with siamese network. \n",
    "# author: Saltanat Khalyk\n",
    "# modified code from @ttchengab ~~~ github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97b2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc997d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82ece94",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4cf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset,Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.utils.data as data_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84267f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pairs of images for inputs, same character label = 1, vice versa\n",
    "class sound_Dataset(Dataset):\n",
    "    def __init__(self, categories, root_dir, transform=None):\n",
    "        self.categories = categories\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        self.audio_names = audio_names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_names)\n",
    "    def __getitem__(self, idx):\n",
    "        audio1 = None\n",
    "        audio2 = None\n",
    "        label = None\n",
    "        \n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        self.audio_names = audio_names\n",
    "        \n",
    "        class_name = self.audio_names[idx].split('\\\\')[1]\n",
    "        list_categories = ['asthma', 'allergies', 'pneumonia', 'covid','other']\n",
    "        channel_dict = {'asthma': 0, 'allergies': 1, 'pneumonia': 2, 'covid':3,'other':4}\n",
    "        label = channel_dict.get(class_name)\n",
    "        \n",
    "        #audio1, sample_rate = torchaudio.load('D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/covid/1629490127741_3.wav')\n",
    "        n_fft = 1024\n",
    "        win_length = None\n",
    "        hop_length = 512\n",
    "        n_mels = 128\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if idx % 2 == 0: # select the same character for both images\n",
    "            category = random.choice(self.categories)\n",
    "            character = random.choice(category[1])\n",
    "            #category = random.choice(list_categories)\n",
    "            audioDir = str(self.root_dir) + str(category[0]) \n",
    "            audio1Name = random.choice(os.listdir(audioDir))\n",
    "            audio2Name = random.choice(os.listdir(audioDir))\n",
    "            audio1, sample_rate = torchaudio.load(str(audioDir) + os.sep + str(audio1Name)) \n",
    "            audio2, sample_rate = torchaudio.load(str(audioDir) + os.sep + str(audio2Name))\n",
    "            mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            melspec1 = mel_spectrogram(audio1)\n",
    "            melspec2 = mel_spectrogram(audio2)\n",
    "            \n",
    "            #print(audioDir + os.sep + audio1Name)\n",
    "            #print(audioDir + os.sep + audio2Name)\n",
    "            label = 1.0\n",
    "        else: # select a different character for both images\n",
    "            category1, category2 = random.choice(self.categories), random.choice(self.categories)\n",
    "            category1, category2 = random.choice(self.categories), random.choice(self.categories)\n",
    "            character1, character2 = random.choice(category1[1]), random.choice(category2[1])\n",
    "            audioDir1, audioDir2 = str(self.root_dir) + str(category1[0]), str(self.root_dir) + str(category2[0])\n",
    "            audio1Name = random.choice(os.listdir(audioDir1))\n",
    "            audio2Name = random.choice(os.listdir(audioDir2))\n",
    "            while audio1Name == audio2Name:\n",
    "                audio2Name = random.choice(os.listdir(audioDir2))\n",
    "            label = 0.0\n",
    "            audio1, sample_rate = torchaudio.load(str(audioDir1) + os.sep + str(audio1Name)) \n",
    "            audio2, sample_rate = torchaudio.load(str(audioDir2) + os.sep+ str(audio2Name))\n",
    "            \n",
    "            mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            melspec1 = mel_spectrogram(audio1)\n",
    "            melspec2 = mel_spectrogram(audio2)\n",
    "#         plt.imshow(img1)\n",
    "        if self.transform:\n",
    "            melspec1 = self.transform(melspec1)\n",
    "            melspec2 = self.transform(melspec2)\n",
    "        return melspec1, melspec2, torch.from_numpy(np.array([label], dtype=np.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c238773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/'\n",
    "path_test = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/test/'\n",
    "path_val = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/val/'\n",
    "#root_dir = path_train\n",
    "categories_train = [[folder, os.listdir(path_train +'/'+ folder)] for folder in os.listdir(path_train)  if not folder.startswith('.') ]\n",
    "#path_val = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/val/'\n",
    "categories_val = [[folder, os.listdir(path_val +'/'+ folder)] for folder in os.listdir(path_val)  if not folder.startswith('.') ]\n",
    "\n",
    "\n",
    "train_set = sound_Dataset(categories_train, path_train, transform =None)\n",
    "val_set = sound_Dataset(categories_val, path_val, transform =None)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aced000",
   "metadata": {},
   "source": [
    "shape of melspec1 for audio 1 - torch.Size([32, 1, 128, 44])\n",
    "\n",
    "\n",
    "shape of melspec2 for audio 2 - torch.Size([32, 1, 128, 44])\n",
    "\n",
    "labels of if audio1=audio2 then 1, else 0 : torch.Size([32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d38bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates n-way one shot learning evaluation\n",
    "class NWayOneShotEvalSet(Dataset):\n",
    "    def __init__(self, categories, root_dir,  numWay, transform=None):\n",
    "        self.categories = categories\n",
    "        self.root_dir = root_dir\n",
    "        #self.setSize = setSize\n",
    "        self.numWay = numWay\n",
    "        self.transform = transform\n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        self.audio_names = audio_names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_names)\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # find one main image\n",
    "        n_fft = 1024\n",
    "        win_length = None\n",
    "        hop_length = 512\n",
    "        n_mels = 128\n",
    "        \n",
    "        category = random.choice(self.categories)\n",
    "        character = random.choice(category[1])\n",
    "        audioDir = str(self.root_dir) + str(category[0])\n",
    "        audioName = random.choice(os.listdir(audioDir))\n",
    "        mainAudio, sample_rate = torchaudio.load(str(audioDir) + os.sep + str(audioName))\n",
    "        \n",
    "        mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        melspec1 = mel_spectrogram(mainAudio)\n",
    "        \n",
    "        \n",
    "        # print(imgDir + '/' + imgName)\n",
    "        if self.transform:\n",
    "            melspec1 = self.transform(melspec1)\n",
    "        \n",
    "        # find n numbers of distinct images, 1 in the same set as the main\n",
    "        testSet = []\n",
    "        label = np.random.randint(self.numWay)\n",
    "        for i in range(self.numWay):\n",
    "            testAudioDir = audioDir\n",
    "            testAudioName = ''\n",
    "            if i == label:\n",
    "                testAudioName = random.choice(os.listdir(audioDir))\n",
    "            else:\n",
    "                testCategory = random.choice(self.categories)\n",
    "                testCharacter = random.choice(testCategory[1])\n",
    "                testAudioDir = self.root_dir + testCategory[0]\n",
    "                while testAudioDir == audioDir:\n",
    "                    testAudioDir = self.root_dir + testCategory[0]\n",
    "                testAudioName = random.choice(os.listdir(testAudioDir))\n",
    "            testAudio, sample_rate = torchaudio.load(str(testAudioDir) + os.sep + str(testAudioName))\n",
    "            \n",
    "            mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            melspec2 = mel_spectrogram(testAudio)\n",
    "            \n",
    "            \n",
    "            if self.transform:\n",
    "                melspec2 = self.transform(melspec2)\n",
    "            \n",
    "            testSet.append(melspec2)\n",
    "        \n",
    "        return melspec1, testSet, torch.from_numpy(np.array([label], dtype = int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59d8d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "numWay =4\n",
    "label = np.random.randint(numWay)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e968e24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character: 1605083054207_5.wav\n",
      "audio DIR:  D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/other\n",
      "Audio name 1605574542532_4.wav\n",
      "main audio torch.Size([1, 22050])\n",
      "Melspc: torch.Size([1, 128, 44])\n",
      "1\n",
      "testAudioDir:  D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/allergies\n",
      "label not equal i 1 0\n",
      "test AUDIO name label equal 1604912115920_2.wav D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/allergies\n",
      "[tensor([[[5.5670e-05, 1.7147e-05, 1.5482e-05,  ..., 9.6861e-05,\n",
      "          9.8773e-05, 9.5619e-05],\n",
      "         [5.0993e-05, 4.3186e-05, 3.2470e-05,  ..., 1.7789e-04,\n",
      "          9.8419e-05, 9.2526e-05],\n",
      "         [4.5146e-05, 8.6817e-05, 6.1082e-05,  ..., 3.1514e-04,\n",
      "          1.0118e-04, 9.0651e-05],\n",
      "         ...,\n",
      "         [1.6850e-09, 1.3878e-09, 1.4954e-09,  ..., 1.8272e-09,\n",
      "          1.6344e-09, 2.5189e-08],\n",
      "         [1.1052e-09, 1.9688e-09, 1.1421e-09,  ..., 1.4110e-09,\n",
      "          1.8511e-09, 2.1684e-08],\n",
      "         [9.7306e-10, 1.6378e-09, 1.2895e-09,  ..., 1.3553e-09,\n",
      "          1.3499e-09, 2.6904e-08]]])]\n",
      "label==i 1 1\n",
      "label equal: 1605007861807_10.wav D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/other\n",
      "[tensor([[[5.5670e-05, 1.7147e-05, 1.5482e-05,  ..., 9.6861e-05,\n",
      "          9.8773e-05, 9.5619e-05],\n",
      "         [5.0993e-05, 4.3186e-05, 3.2470e-05,  ..., 1.7789e-04,\n",
      "          9.8419e-05, 9.2526e-05],\n",
      "         [4.5146e-05, 8.6817e-05, 6.1082e-05,  ..., 3.1514e-04,\n",
      "          1.0118e-04, 9.0651e-05],\n",
      "         ...,\n",
      "         [1.6850e-09, 1.3878e-09, 1.4954e-09,  ..., 1.8272e-09,\n",
      "          1.6344e-09, 2.5189e-08],\n",
      "         [1.1052e-09, 1.9688e-09, 1.1421e-09,  ..., 1.4110e-09,\n",
      "          1.8511e-09, 2.1684e-08],\n",
      "         [9.7306e-10, 1.6378e-09, 1.2895e-09,  ..., 1.3553e-09,\n",
      "          1.3499e-09, 2.6904e-08]]]), tensor([[[5.9615e-01, 6.6088e+00, 2.4635e+00,  ..., 1.8051e+00,\n",
      "          9.4424e+00, 3.3237e+00],\n",
      "         [1.0824e+00, 4.8303e+00, 2.0108e+00,  ..., 1.8060e+00,\n",
      "          6.2870e+00, 2.3903e+00],\n",
      "         [1.9065e+00, 2.1136e+00, 1.3458e+00,  ..., 1.8685e+00,\n",
      "          1.3898e+00, 9.5971e-01],\n",
      "         ...,\n",
      "         [6.5970e-08, 1.1004e-09, 1.3329e-09,  ..., 9.9689e-10,\n",
      "          2.1909e-09, 5.9684e-07],\n",
      "         [4.9304e-08, 1.2672e-09, 1.9247e-09,  ..., 2.2378e-09,\n",
      "          1.3776e-09, 5.9102e-07],\n",
      "         [4.8568e-08, 1.8840e-09, 1.5278e-09,  ..., 1.0855e-09,\n",
      "          1.1585e-09, 5.5793e-07]]])]\n"
     ]
    }
   ],
   "source": [
    "root_dir = path_train\n",
    "numWay=2\n",
    "categories = [[folder, os.listdir(path_train +'/'+ folder)] for folder in os.listdir(path_train)  if not folder.startswith('.') ]\n",
    "category = random.choice(categories)\n",
    "#print('category',category, '/n')\n",
    "character = random.choice(category[1])\n",
    "print('character:' , character)\n",
    "audioDir = str(root_dir) + str(category[0])\n",
    "print('audio DIR: ',audioDir)\n",
    "audioName = random.choice(os.listdir(audioDir))\n",
    "print('Audio name',audioName)\n",
    "mainAudio, sample_rate = torchaudio.load(str(audioDir) + os.sep + str(audioName))\n",
    "print('main audio', mainAudio.size())\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 128        \n",
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm=\"slaney\",\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "melspec1 = mel_spectrogram(mainAudio)\n",
    "print('Melspc:', melspec1.size())       \n",
    "        \n",
    "        # print(imgDir + '/' + imgName)\n",
    "#if self.transform:\n",
    "    #melspec1 = self.transform(melspec1)\n",
    "        \n",
    "        # find n numbers of distinct images, 1 in the same set as the main\n",
    "testSet = []\n",
    "label = np.random.randint(numWay)\n",
    "print(label)\n",
    "for i in range(numWay):\n",
    "    testAudioDir = audioDir\n",
    "    testAudioName = ''\n",
    "    if i == label:\n",
    "        print('label==i', label, i)\n",
    "        testAudioName = random.choice(os.listdir(audioDir))\n",
    "        print('label equal:',testAudioName, testAudioDir)\n",
    "    else:\n",
    "        testCategory = random.choice(categories)\n",
    "        testCharacter = random.choice(testCategory[1])\n",
    "        testAudioDir = root_dir + testCategory[0]\n",
    "        print('testAudioDir: ',testAudioDir)\n",
    "        print('label not equal i', label, i)\n",
    "        while testAudioDir == audioDir:\n",
    "            testAudioDir = root_dir + testCategory[0]\n",
    "        testAudioName = random.choice(os.listdir(testAudioDir))\n",
    "        print('test AUDIO name label equal', testAudioName, testAudioDir)\n",
    "    testAudio, sample_rate = torchaudio.load(str(testAudioDir) +os.sep+ str(testAudioName))\n",
    "            \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        onesided=True,\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",)\n",
    "            \n",
    "    melspec2 = mel_spectrogram(testAudio)\n",
    "            \n",
    "            \n",
    "            \n",
    "    testSet.append(melspec2)\n",
    "    print(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "790a1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "numWay = 1\n",
    "\n",
    "\n",
    "path_train = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/'\n",
    "path_test = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/test/'\n",
    "#root_dir = path_train\n",
    "categories = [[folder, os.listdir(path_test +'/'+ folder)] for folder in os.listdir(path_test)  if not folder.startswith('.') ]\n",
    "\n",
    "test_set = NWayOneShotEvalSet(categories, path_test, numWay, transform=None)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, num_workers = 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60eae45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for melspec1,testSet, labels in (test_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #for i, melspec2 in enumerate(testSet):\n",
    "            #print(melspec2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "523aa1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c8c75",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "\n",
    "(background infa: I have huge tensors for audio files, and they take a lot of memory,  so I want to reduce the size and extract only useful features to feed them to cnn)\n",
    "\n",
    "how to extract audio features (in pytorch only)? \n",
    "\n",
    "\n",
    "1. extract mel frequency?\n",
    "2. or use pretrained torchaudio.models (emformer, convTasNet) - is it useful when the audio is not speech to text conversion problem?\n",
    "3. or use pretrained torchaudio.pipelines? (also not asr task, is it useful?)\n",
    "\n",
    "what is the good approach (1.2.3)? or all of them are good/bad? or what other methods could u recommend?\n",
    "\n",
    "#audioacousticfeaturextraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3220a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72b090fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract mfcc features from audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc2b9a",
   "metadata": {},
   "source": [
    "## Can I use any other cnn for the siamese network? \n",
    "The original siamese net for one-shot-learning was for 4d input, whereas we have 3d. \n",
    "so I modified the network slightly, but the it took a lot of memory. For example can I try something smaller and apply sigmoid function in the end??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb8fb2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different network structures, the commented out are the different experimenting structures\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Koch et al.\n",
    "        # Conv2d(input_channels, output_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3) \n",
    "        self.conv2 = nn.Conv2d(2, 5, 3)  \n",
    "        self.conv3 = nn.Conv2d(5, 10, 1)\n",
    "        self.conv4 = nn.Conv2d(10, 16, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(2)\n",
    "        self.bn2 = nn.BatchNorm2d(5)\n",
    "        self.bn3 = nn.BatchNorm2d(10)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(960,32)\n",
    "        self.fcOut = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # VGG16\n",
    "        # # dataiter = iter(train_loader)\n",
    "        # # img1, img2, label = dataiter.next()\n",
    "        # # print(img1.shape)\n",
    "        # self.conv11 = nn.Conv2d(1, 64, 3) \n",
    "        # self.conv12 = nn.Conv2d(64, 64, 3)  \n",
    "        # self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        # self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "        # self.conv31 = nn.Conv2d(128, 256, 3) \n",
    "        # self.conv32 = nn.Conv2d(256, 256, 3)  \n",
    "        # self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.fc1 = nn.Linear(256 * 8 * 8, 4096)\n",
    "        # self.fc2 = nn.Linear(4096, 4096)\n",
    "        # self.fcOut = nn.Linear(4096, 1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        # # x = self.conv11(img1)\n",
    "        # # x = self.conv12(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # x = self.conv21(x)\n",
    "        # # x = self.conv22(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # x = self.conv31(x)\n",
    "        # # x = self.conv32(x)\n",
    "        # # x = self.conv33(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # print(x.shape)\n",
    "    \n",
    "    def convs(self, x):\n",
    "\n",
    "        # Koch et al.\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        #1, 105, 105\n",
    "        x = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        # 64, 96, 96\n",
    "        x = torch.nn.functional.max_pool2d(x, (2,2))\n",
    "        # 64, 48, 48\n",
    "        x = torch.nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        # 128, 42, 42\n",
    "        x = torch.nn.functional.max_pool2d(x, (2,2))\n",
    "        # 128, 21, 21\n",
    "        x = torch.nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        # 128, 18, 18\n",
    "        x = torch.nn.functional.max_pool2d(x, (2,2))\n",
    "        # 128, 9, 9\n",
    "        x = torch.nn.functional.relu(self.bn4(self.conv4(x)))\n",
    "        # 256, 6, 6\n",
    "        return x\n",
    "\n",
    "        # VGG16\n",
    "        # x = F.relu(self.conv11(x))\n",
    "        # x = F.relu(self.conv12(x))\n",
    "        # x = F.max_pool2d(x, (2,2))\n",
    "        # x = F.relu(self.conv21(x))\n",
    "        # x = F.relu(self.conv22(x))\n",
    "        # x = F.max_pool2d(x, (2,2))\n",
    "        # x = F.relu(self.conv31(x))\n",
    "        # x = F.relu(self.conv32(x))\n",
    "        # x = F.relu(self.conv33(x))\n",
    "        # x = F.max_pool2d(x, (2,2))\n",
    "        # return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "\n",
    "        # Koch et al.\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x1 = self.sigmoid(self.fc1(x1))\n",
    "\n",
    "        # VGG16\n",
    "        # x1 = x1.view(-1, 256 * 8 * 8)\n",
    "        # x1 = self.fc1(x1)\n",
    "        # x1 = self.sigmoid(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "\n",
    "        # Koch et al.\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x2 = self.sigmoid(self.fc1(x2))\n",
    "\n",
    "        # VGG16\n",
    "        # x2 = x2.view(-1, 256 * 8 * 8)\n",
    "        # x2 = self.fc1(x2)\n",
    "        # x2 = self.sigmoid(self.fc2(x2))\n",
    "\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fcOut(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0eb8a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGSiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(1, 64, 3) \n",
    "        self.conv12 = nn.Conv2d(64, 64, 3)  \n",
    "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "        self.conv31 = nn.Conv2d(128, 256, 3) \n",
    "        self.conv32 = nn.Conv2d(256, 256, 3)  \n",
    "        self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fcOut = nn.Linear(4096, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def convs(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.conv22(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv31(x))\n",
    "        x = F.relu(self.conv32(x))\n",
    "        x = F.relu(self.conv33(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256 * 8 * 8)\n",
    "        x1 = self.fc1(x1)\n",
    "        x1 = self.sigmoid(self.fc2(x1))\n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256 * 8 * 8)\n",
    "        x2 = self.fc1(x2)\n",
    "        x2 = self.sigmoid(self.fc2(x2))\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fcOut(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0b59af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for melspec1, testSet, label in test_loader:\n",
    "            melspec1 = melspec1.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            \n",
    "            # determine which category an image belongs to\n",
    "            for i, melspec2 in enumerate(testSet):\n",
    "                melspec2 = melspec2.to(device)\n",
    "                output = model(melspec1, melspec2)\n",
    "                if output > predVal:\n",
    "                    pred = i\n",
    "                    predVal = output\n",
    "            label = label.to(device)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "            if count % 20 == 0:\n",
    "                print(\"Current Count is: {}\".format(count))\n",
    "                print('Accuracy on n way: {}'.format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de66a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model network: 99 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "#count\n",
    "with torch.no_grad():\n",
    "    for melspec1, testSet, label in test_loader:\n",
    "        #melspec1,testSet, label = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "            #melspec1, testSet, label = melspec1.to(device), testSet.to(device), label.to(device)\n",
    "        melspec1 = melspec1.to(device)\n",
    "        label =label.to(device)\n",
    "        predVal = 0\n",
    "        pred = -1\n",
    "        for i, melspec2 in enumerate(testSet):\n",
    "                \n",
    "            melspec2 = melspec2.to(device)\n",
    "                \n",
    "            outputs = load_model(melspec1, melspec2)\n",
    "            if outputs > predVal:\n",
    "                pred=i\n",
    "                predVal = outputs\n",
    "        if pred==label:\n",
    "            correct+=1\n",
    "        \n",
    "        total += label.size(0)\n",
    "        #correct += (predicted == label).sum().item()\n",
    "accr = (100 * correct / total)\n",
    "print('Accuracy of the model network: %d %%' % (accr))\n",
    "\n",
    "## 1shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "#count\n",
    "with torch.no_grad():\n",
    "    for melspec1, testSet, label in test_loader:\n",
    "        #melspec1,testSet, label = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "            #melspec1, testSet, label = melspec1.to(device), testSet.to(device), label.to(device)\n",
    "        melspec1 = melspec1.to(device)\n",
    "        label =label.to(device)\n",
    "        predVal = 0\n",
    "        pred = -1\n",
    "        for i, melspec2 in enumerate(testSet):\n",
    "                \n",
    "            melspec2 = melspec2.to(device)\n",
    "                \n",
    "            outputs = load_model(melspec1, melspec2)\n",
    "            if outputs > predVal:\n",
    "                pred=i\n",
    "                predVal = outputs\n",
    "        if pred==label:\n",
    "            correct+=1\n",
    "        \n",
    "        total += label.size(0)\n",
    "        #correct += (predicted == label).sum().item()\n",
    "accr = (100 * correct / total)\n",
    "print('Accuracy of the model network: %d %%' % (accr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fce020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model architecture:\n",
      "\n",
      " Net(\n",
      "  (conv1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(5, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=960, out_features=32, bias=True)\n",
      "  (fcOut): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "The model has 31,202 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "siameseBaseLine = Net()\n",
    "siameseBaseLine = siameseBaseLine.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model architecture:\\n\\n', model)\n",
    "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
    "    \n",
    "count_parameters(siameseBaseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5128372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading checkpoint mechanisms\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "    if save_path==None:\n",
    "        return\n",
    "    save_path = save_path \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_loss': val_loss}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(model, optimizer):\n",
    "    save_path = f'siameseNet-batchnorm50.pt'\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    val_loss = state_dict['val_loss']\n",
    "    print(f'Model loaded from <== {save_path}')\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00587613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs, criterion, save_name):\n",
    "    best_val_loss = float(\"Inf\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    cur_step = 0\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        print(\"Starting epoch \" + str(epoch+1))\n",
    "        for audio1, audio2, labels in train_loader:\n",
    "            \n",
    "            # Forward\n",
    "            audio1 = audio1.to(device)\n",
    "            audio2 = audio2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(audio1, audio2)\n",
    "            outputs.to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            for audio1, audio2, labels in val_loader:\n",
    "                audio1 = audio1.to(device)\n",
    "                audio2 = audio2.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(audio1, audio2)\n",
    "                outputs.to(device)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
    "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "    \n",
    "    print(\"Finished Training\")  \n",
    "    return train_losses, val_losses  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c03db",
   "metadata": {},
   "source": [
    "# evaluation metrics\n",
    "def evaluation(model, test_loader):\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        print('Starting Iteration')\n",
    "        count = 0\n",
    "        for mainAudio, testSet, label in test_loader:\n",
    "            mainAudio = mainAudio.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            for i, testAudio in enumerate(testSet):\n",
    "                testAudio = testAudio.to(device)\n",
    "                output = model(mainAudio, testAudio)\n",
    "                output.to(device)\n",
    "                \n",
    "                \n",
    "                if output > predVal:\n",
    "                    pred = i\n",
    "                    predVal = output\n",
    "            label = label.to(device)\n",
    "            #pred.to(device)\n",
    "            if pred == label:\n",
    "                \n",
    "                correct += 1\n",
    "            count += 1\n",
    "            if count % 20 == 0:\n",
    "                print(\"Current Count is: {}\".format(count))\n",
    "                \n",
    "                print('Accuracy on n way: {}'.format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7fa88eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch [1/2],Train Loss: 0.4478, Valid Loss: 0.38524995\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 2\n",
      "Epoch [2/2],Train Loss: 0.3809, Valid Loss: 0.36068033\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# actual training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.0006)\n",
    "num_epochs = 2\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "save_path = 'siameseNet-batchnorm50.pt'\n",
    "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae6c552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch [1/2],Train Loss: 0.3402, Valid Loss: 0.36162573\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 2\n",
      "Epoch [2/2],Train Loss: 0.3239, Valid Loss: 0.32928113\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# actual training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.0006)\n",
    "num_epochs = 2\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "save_path = 'siameseNet-batchnorm50.pt'\n",
    "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97b4b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ccd925da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on previously saved models\n",
    "import torch.optim as optim\n",
    "load_model = Net().to(device)\n",
    "load_optimizer = optim.Adam(load_model.parameters(), lr=0.0006)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd772b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== siameseNet-batchnorm50.pt\n",
      "0.3292811316174088\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "eval_every = 1000\n",
    "total_step = len(train_loader)*num_epochs\n",
    "best_val_loss = load_checkpoint(load_model, load_optimizer)\n",
    "\n",
    "print(best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41c8663a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_876/923759202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_876/2035536255.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmelspec1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mmelspec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmelspec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mpredVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_876/3904378805.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mtestAudioDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtestCategory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0mtestAudioDir\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0maudioDir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                     \u001b[0mtestAudioDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtestCategory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[0mtestAudioName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestAudioDir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mtestAudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestAudioDir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestAudioName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(load_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c251f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d21954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"How to use the evaluation n way:\n",
    "\n",
    "# Set the parameters\n",
    "testSize = 5000 # how big you want your test size to be\n",
    "numWay = 4 # how many ways metric\n",
    "\n",
    "# Create the dataset for it and put it into dataloader\n",
    "test_set = NWayOneShotEvalSet(categories, root_dir, testSize, numWay, transformations) \n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, num_workers = 2)\n",
    "\n",
    "\n",
    "# Load the models (the name of the loaded model can be changed in the load_checkpoint() function)\n",
    "load_model = Net().to(device)\n",
    "load_optimizer = optim.Adam(load_model.parameters(), lr=0.0006)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "eval_every = 1000\n",
    "total_step = len(train_loader)*num_epochs\n",
    "best_val_loss = load_checkpoint(load_model, load_optimizer)\n",
    "\n",
    "print(best_val_loss)\n",
    "\n",
    "# Evaluate from the test loader \n",
    "\n",
    "eval(load_model, test_loader)\n",
    "\n",
    "\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d6a54d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5d3f5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12524/1965261682.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnvidia\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msmi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5306f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  1 15:14:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.41       Driver Version: 471.41       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0 Off |                  N/A |\n",
      "| N/A   50C    P8     8W /  N/A |   1101MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4884      C   ...\\envs\\torchgpu\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78094fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
