{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1009b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One shot learning for audio with siamese network. \n",
    "# author: Saltanat Khalyk\n",
    "# modified code from @ttchengab ~~~ github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97b2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc997d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82ece94",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4cf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset,Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.utils.data as data_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84267f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pairs of images for inputs, same character label = 1, vice versa\n",
    "class sound_Dataset(Dataset):\n",
    "    def __init__(self, categories, root_dir, transform=None):\n",
    "        self.categories = categories\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        self.audio_names = audio_names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_names)\n",
    "    def __getitem__(self, idx):\n",
    "        audio1 = None\n",
    "        audio2 = None\n",
    "        label = None\n",
    "        \n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        self.audio_names = audio_names\n",
    "        \n",
    "        class_name = self.audio_names[idx].split('\\\\')[1]\n",
    "        list_categories = ['asthma', 'allergies', 'pneumonia', 'covid','other']\n",
    "        channel_dict = {'asthma': 0, 'allergies': 1, 'pneumonia': 2, 'covid':3,'other':4}\n",
    "        label = channel_dict.get(class_name)\n",
    "        \n",
    "        #audio1, sample_rate = torchaudio.load('D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/covid/1629490127741_3.wav')\n",
    "        n_fft = 1024\n",
    "        win_length = None\n",
    "        hop_length = 512\n",
    "        n_mels = 128\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if idx % 2 == 0: # select the same character for both images\n",
    "            category = random.choice(self.categories)\n",
    "            character = random.choice(category[1])\n",
    "            #category = random.choice(list_categories)\n",
    "            audioDir = str(self.root_dir) + str(category[0]) \n",
    "            audio1Name = random.choice(os.listdir(audioDir))\n",
    "            audio2Name = random.choice(os.listdir(audioDir))\n",
    "            audio1, sample_rate = torchaudio.load(str(audioDir) + os.sep + str(audio1Name)) \n",
    "            audio2, sample_rate = torchaudio.load(str(audioDir) + os.sep + str(audio2Name))\n",
    "            mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            melspec1 = mel_spectrogram(audio1)\n",
    "            melspec2 = mel_spectrogram(audio2)\n",
    "            \n",
    "            #print(audioDir + os.sep + audio1Name)\n",
    "            #print(audioDir + os.sep + audio2Name)\n",
    "            label = 1.0\n",
    "        else: # select a different character for both images\n",
    "            category1, category2 = random.choice(self.categories), random.choice(self.categories)\n",
    "            category1, category2 = random.choice(self.categories), random.choice(self.categories)\n",
    "            character1, character2 = random.choice(category1[1]), random.choice(category2[1])\n",
    "            audioDir1, audioDir2 = str(self.root_dir) + str(category1[0]), str(self.root_dir) + str(category2[0])\n",
    "            audio1Name = random.choice(os.listdir(audioDir1))\n",
    "            audio2Name = random.choice(os.listdir(audioDir2))\n",
    "            while audio1Name == audio2Name:\n",
    "                audio2Name = random.choice(os.listdir(audioDir2))\n",
    "            label = 0.0\n",
    "            audio1, sample_rate = torchaudio.load(str(audioDir1) + os.sep + str(audio1Name)) \n",
    "            audio2, sample_rate = torchaudio.load(str(audioDir2) + os.sep+ str(audio2Name))\n",
    "            \n",
    "            mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            melspec1 = mel_spectrogram(audio1)\n",
    "            melspec2 = mel_spectrogram(audio2)\n",
    "#         plt.imshow(img1)\n",
    "        if self.transform:\n",
    "            melspec1 = self.transform(melspec1)\n",
    "            melspec2 = self.transform(melspec2)\n",
    "        return melspec1, melspec2, torch.from_numpy(np.array([label], dtype=np.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c238773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/'\n",
    "path_test = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/test/'\n",
    "path_val = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/val/'\n",
    "#root_dir = path_train\n",
    "categories_train = [[folder, os.listdir(path_train +'/'+ folder)] for folder in os.listdir(path_train)  if not folder.startswith('.') ]\n",
    "#path_val = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/val/'\n",
    "categories_val = [[folder, os.listdir(path_val +'/'+ folder)] for folder in os.listdir(path_val)  if not folder.startswith('.') ]\n",
    "\n",
    "\n",
    "train_set = sound_Dataset(categories_train, path_train, transform =None)\n",
    "val_set = sound_Dataset(categories_val, path_val, transform =None)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed11c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "988e6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataloader works properly and the shape your AUDIO FILES\n",
    "#for i, batch in enumerate(train_loader):\n",
    "    #for i, x in enumerate(batch):\n",
    "        #print(x.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aced000",
   "metadata": {},
   "source": [
    "shape of melspec1 for audio 1 - torch.Size([32, 1, 128, 44])\n",
    "\n",
    "\n",
    "shape of melspec2 for audio 2 - torch.Size([32, 1, 128, 44])\n",
    "\n",
    "labels of if audio1=audio2 then 1, else 0 : torch.Size([32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d38bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates n-way one shot learning evaluation\n",
    "class NWayOneShotEvalSet(Dataset):\n",
    "    def __init__(self, categories, root_dir,  numWay, transform=None):\n",
    "        self.categories = categories\n",
    "        self.root_dir = root_dir\n",
    "        #self.setSize = setSize\n",
    "        self.numWay = numWay\n",
    "        self.transform = transform\n",
    "        audio_names =[]\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                filepath =subdir+os.sep+file\n",
    "                audio_names.append(filepath)\n",
    "        self.audio_names = audio_names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_names)\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # find one main image\n",
    "        n_fft = 1024\n",
    "        win_length = None\n",
    "        hop_length = 512\n",
    "        n_mels = 128\n",
    "        \n",
    "        category = random.choice(categories)\n",
    "        character = random.choice(category[1])\n",
    "        audioDir = str(self.root_dir) + str(category[0])\n",
    "        audioName = random.choice(os.listdir(audioDir))\n",
    "        mainAudio, sample_rate = torchaudio.load(str(audioDir) + os.sep + str(audioName))\n",
    "        \n",
    "        mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        melspec1 = mel_spectrogram(mainAudio)\n",
    "        \n",
    "        \n",
    "        # print(imgDir + '/' + imgName)\n",
    "        if self.transform:\n",
    "            melspec1 = self.transform(melspec1)\n",
    "        \n",
    "        # find n numbers of distinct images, 1 in the same set as the main\n",
    "        testSet = []\n",
    "        label = np.random.randint(self.numWay)\n",
    "        for i in range(self.numWay):\n",
    "            testAudioDir = audioDir\n",
    "            testAudioName = ''\n",
    "            if i == label:\n",
    "                testAudioName = random.choice(os.listdir(audioDir))\n",
    "            else:\n",
    "                testCategory = random.choice(categories)\n",
    "                testCharacter = random.choice(testCategory[1])\n",
    "                testAudioDir = self.root_dir + testCategory[0]\n",
    "                while testAudioDir == audioDir:\n",
    "                    testAudioDir = self.root_dir + testCategory[0]\n",
    "                testAudioName = random.choice(os.listdir(testAudioDir))\n",
    "            testAudio, sample_rate = torchaudio.load(str(testAudioDir) + os.sep + str(testAudioName))\n",
    "            \n",
    "            mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",)\n",
    "            \n",
    "            melspec2 = mel_spectrogram(testAudio)\n",
    "            \n",
    "            \n",
    "            if self.transform:\n",
    "                melspec2 = self.transform(melspec2)\n",
    "            testSet.append(melspec2)\n",
    "        \n",
    "        return melspec1, testSet, torch.from_numpy(np.array([label], dtype = int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "790a1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "numWay = 4\n",
    "\n",
    "\n",
    "path_train = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/'\n",
    "path_test = 'D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/test/'\n",
    "#root_dir = path_train\n",
    "categories = [[folder, os.listdir(path_test +'/'+ folder)] for folder in os.listdir(path_test)  if not folder.startswith('.') ]\n",
    "\n",
    "test_set = NWayOneShotEvalSet(categories, path_test, numWay, transform=None)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, num_workers = 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "112def63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = [[folder, os.listdir(path_test +'/'+ folder)] for folder in os.listdir(path_test)  if not folder.startswith('.') ]\n",
    "\n",
    "#category = random.choice(categories)\n",
    "#character = random.choice(category[1])\n",
    "#audioDir = str(root_dir) + str(category[0])\n",
    "#print(audioDir)\n",
    "#audioName = random.choice(os.listdir(audioDir))\n",
    "#mainAudio, _ = torchaudio.load(str(audioDir) + os.sep + str(audioName))\n",
    "#print(mainAudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a24bc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories_test = [[folder, os.listdir(path_test +'/'+ folder)] for folder in os.listdir(path_test)  if not folder.startswith('.') ]\n",
    "\n",
    "#testCategory = random.choice(categories_test)\n",
    "\n",
    "#testCharacter = random.choice(testCategory[1])\n",
    "#testAudioDir = root_dir + testCategory[0]\n",
    "#print(testAudioDir)\n",
    "#testAudioName = random.choice(os.listdir(testAudioDir))\n",
    "#print(testAudioName)\n",
    "#testAudio, _ = torchaudio.load(str(testAudioDir) + os.sep + str(testAudioName))\n",
    "\n",
    "#print(testAudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "523aa1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9bb0f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i_batch, sample_batched in enumerate(test_loader):\n",
    "    #print(i_batch, sample_batched[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c8c75",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "\n",
    "(background infa: I have huge tensors for audio files, and they take a lot of memory,  so I want to reduce the size and extract only useful features to feed them to cnn)\n",
    "\n",
    "how to extract audio features (in pytorch only)? \n",
    "\n",
    "\n",
    "1. extract mel frequency?\n",
    "2. or use pretrained torchaudio.models (emformer, convTasNet) - is it useful when the audio is not speech to text conversion problem?\n",
    "3. or use pretrained torchaudio.pipelines? (also not asr task, is it useful?)\n",
    "\n",
    "what is the good approach (1.2.3)? or all of them are good/bad? or what other methods could u recommend?\n",
    "\n",
    "#audioacousticfeaturextraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3220a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72b090fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract mfcc features from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3353f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load('D:/COVID_COUGH_SOUNDS/1second_chunks_458/balanced_1s_not_aug/one_shot1sec/train/covid/1629490127741_3.wav')\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 128\n",
    "\n",
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm=\"slaney\",\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    ")\n",
    "\n",
    "melspec = mel_spectrogram(waveform)\n",
    "#plot_spectrogram(melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9bf5acda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 44])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6657d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavenet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc2b9a",
   "metadata": {},
   "source": [
    "## Can I use any other cnn for the siamese network? \n",
    "The original siamese net for one-shot-learning was for 4d input, whereas we have 3d. \n",
    "so I modified the network slightly, but the it took a lot of memory. For example can I try something smaller and apply sigmoid function in the end??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fb8fb2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different network structures, the commented out are the different experimenting structures\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Koch et al.\n",
    "        # Conv2d(input_channels, output_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3) \n",
    "        self.conv2 = nn.Conv2d(2, 5, 3)  \n",
    "        self.conv3 = nn.Conv2d(5, 10, 1)\n",
    "        self.conv4 = nn.Conv2d(10, 16, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(2)\n",
    "        self.bn2 = nn.BatchNorm2d(5)\n",
    "        self.bn3 = nn.BatchNorm2d(10)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(960,32)\n",
    "        self.fcOut = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # VGG16\n",
    "        # # dataiter = iter(train_loader)\n",
    "        # # img1, img2, label = dataiter.next()\n",
    "        # # print(img1.shape)\n",
    "        # self.conv11 = nn.Conv2d(1, 64, 3) \n",
    "        # self.conv12 = nn.Conv2d(64, 64, 3)  \n",
    "        # self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        # self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "        # self.conv31 = nn.Conv2d(128, 256, 3) \n",
    "        # self.conv32 = nn.Conv2d(256, 256, 3)  \n",
    "        # self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.fc1 = nn.Linear(256 * 8 * 8, 4096)\n",
    "        # self.fc2 = nn.Linear(4096, 4096)\n",
    "        # self.fcOut = nn.Linear(4096, 1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        # # x = self.conv11(img1)\n",
    "        # # x = self.conv12(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # x = self.conv21(x)\n",
    "        # # x = self.conv22(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # x = self.conv31(x)\n",
    "        # # x = self.conv32(x)\n",
    "        # # x = self.conv33(x)\n",
    "        # # x = self.pool(x)\n",
    "        # # print(x.shape)\n",
    "    \n",
    "    def convs(self, x):\n",
    "\n",
    "        # Koch et al.\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        #1, 105, 105\n",
    "        x = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        # 64, 96, 96\n",
    "        x = torch.nn.functional.max_pool2d(x, (2,2))\n",
    "        # 64, 48, 48\n",
    "        x = torch.nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        # 128, 42, 42\n",
    "        x = torch.nn.functional.max_pool2d(x, (2,2))\n",
    "        # 128, 21, 21\n",
    "        x = torch.nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        # 128, 18, 18\n",
    "        x = torch.nn.functional.max_pool2d(x, (2,2))\n",
    "        # 128, 9, 9\n",
    "        x = torch.nn.functional.relu(self.bn4(self.conv4(x)))\n",
    "        # 256, 6, 6\n",
    "        return x\n",
    "\n",
    "        # VGG16\n",
    "        # x = F.relu(self.conv11(x))\n",
    "        # x = F.relu(self.conv12(x))\n",
    "        # x = F.max_pool2d(x, (2,2))\n",
    "        # x = F.relu(self.conv21(x))\n",
    "        # x = F.relu(self.conv22(x))\n",
    "        # x = F.max_pool2d(x, (2,2))\n",
    "        # x = F.relu(self.conv31(x))\n",
    "        # x = F.relu(self.conv32(x))\n",
    "        # x = F.relu(self.conv33(x))\n",
    "        # x = F.max_pool2d(x, (2,2))\n",
    "        # return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "\n",
    "        # Koch et al.\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x1 = self.sigmoid(self.fc1(x1))\n",
    "\n",
    "        # VGG16\n",
    "        # x1 = x1.view(-1, 256 * 8 * 8)\n",
    "        # x1 = self.fc1(x1)\n",
    "        # x1 = self.sigmoid(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "\n",
    "        # Koch et al.\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x2 = self.sigmoid(self.fc1(x2))\n",
    "\n",
    "        # VGG16\n",
    "        # x2 = x2.view(-1, 256 * 8 * 8)\n",
    "        # x2 = self.fc1(x2)\n",
    "        # x2 = self.sigmoid(self.fc2(x2))\n",
    "\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fcOut(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGSiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(1, 64, 3) \n",
    "        self.conv12 = nn.Conv2d(64, 64, 3)  \n",
    "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "        self.conv31 = nn.Conv2d(128, 256, 3) \n",
    "        self.conv32 = nn.Conv2d(256, 256, 3)  \n",
    "        self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fcOut = nn.Linear(4096, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def convs(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.conv22(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv31(x))\n",
    "        x = F.relu(self.conv32(x))\n",
    "        x = F.relu(self.conv33(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256 * 8 * 8)\n",
    "        x1 = self.fc1(x1)\n",
    "        x1 = self.sigmoid(self.fc2(x1))\n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256 * 8 * 8)\n",
    "        x2 = self.fc1(x2)\n",
    "        x2 = self.sigmoid(self.fc2(x2))\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fcOut(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809c56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for melspec1, testSet, label in test_loader:\n",
    "            melspec1 = melspec1.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            \n",
    "            # determine which category an image belongs to\n",
    "            for i, melspec2 in enumerate(testSet):\n",
    "                melspec2 = melspec2.to(device)\n",
    "                output = model(melspec1, melspec2)\n",
    "                if output > predVal:\n",
    "                    pred = i\n",
    "                    predVal = output\n",
    "            label = label.to(device)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "            if count % 20 == 0:\n",
    "                print(\"Current Count is: {}\".format(count))\n",
    "                print('Accuracy on n way: {}'.format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fce020b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9140/4292654783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msiameseBaseLine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msiameseBaseLine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msiameseBaseLine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcount_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "siameseBaseLine = Net()\n",
    "siameseBaseLine = siameseBaseLine.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model architecture:\\n\\n', model)\n",
    "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
    "    \n",
    "count_parameters(siameseBaseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b5128372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading checkpoint mechanisms\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "    if save_path==None:\n",
    "        return\n",
    "    save_path = save_path \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_loss': val_loss}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(model, optimizer):\n",
    "    save_path = f'siameseNet-batchnorm50.pt'\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    val_loss = state_dict['val_loss']\n",
    "    print(f'Model loaded from <== {save_path}')\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "00587613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs, criterion, save_name):\n",
    "    best_val_loss = float(\"Inf\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    cur_step = 0\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        print(\"Starting epoch \" + str(epoch+1))\n",
    "        for audio1, audio2, labels in train_loader:\n",
    "            \n",
    "            # Forward\n",
    "            audio1 = audio1.to(device)\n",
    "            audio2 = audio2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(audio1, audio2)\n",
    "            outputs.to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            for audio1, audio2, labels in val_loader:\n",
    "                audio1 = audio1.to(device)\n",
    "                audio2 = audio2.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(audio1, audio2)\n",
    "                outputs.to(device)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
    "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "    \n",
    "    print(\"Finished Training\")  \n",
    "    return train_losses, val_losses  \n",
    "\n",
    "# evaluation metrics\n",
    "def evaluation(model, test_loader):\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        print('Starting Iteration')\n",
    "        count = 0\n",
    "        for mainAudio, testSet, label in test_loader:\n",
    "            mainAudio = mainAudio.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            for i, testAudio in enumerate(testSet):\n",
    "                testAudio = testAudio.to(device)\n",
    "                output = model(mainAudio, testAudio)\n",
    "                output.to(device)\n",
    "                \n",
    "                \n",
    "                if output > predVal:\n",
    "                    pred = i\n",
    "                    predVal = output\n",
    "            label = label.to(device)\n",
    "            #pred.to(device)\n",
    "            if pred == label:\n",
    "                \n",
    "                correct += 1\n",
    "            count += 1\n",
    "            if count % 20 == 0:\n",
    "                print(\"Current Count is: {}\".format(count))\n",
    "                \n",
    "                print('Accuracy on n way: {}'.format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f3b75a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "#for mainAudio, testSet, label in test_loader:\n",
    "    #print(len(testSet))\n",
    "    #print(label)\n",
    "    #print(len(testSet))\n",
    "    #if label != 1:\n",
    "        #for i, testAudio in enumerate(testSet):\n",
    "          #plt.subplot(1, len(imgset)+1, count+1)\n",
    "          #plt.imshow(img[0][0])\n",
    "            #print(testAudio.shape)\n",
    "        #print(mainAudio.shape)\n",
    "        #plt.subplot(1, len(imgset)+1, len(imgset)+1)\n",
    "        #plt.imshow(mainImg[0][0])\n",
    "        #i+= 1\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fa88eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch [1/5],Train Loss: 0.3191, Valid Loss: 0.28640259\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 2\n",
      "Epoch [2/5],Train Loss: 0.2840, Valid Loss: 0.29354476\n",
      "Starting epoch 3\n",
      "Epoch [3/5],Train Loss: 0.2777, Valid Loss: 0.29391629\n",
      "Starting epoch 4\n",
      "Epoch [4/5],Train Loss: 0.2756, Valid Loss: 0.27606322\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 5\n",
      "Epoch [5/5],Train Loss: 0.2690, Valid Loss: 0.26808680\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# actual training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.0006)\n",
    "num_epochs = 5\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "save_path = 'siameseNet-batchnorm50.pt'\n",
    "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ae6c552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch [1/10],Train Loss: 0.2660, Valid Loss: 0.25703109\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 2\n",
      "Epoch [2/10],Train Loss: 0.2609, Valid Loss: 0.26828378\n",
      "Starting epoch 3\n",
      "Epoch [3/10],Train Loss: 0.2567, Valid Loss: 0.26283369\n",
      "Starting epoch 4\n",
      "Epoch [4/10],Train Loss: 0.2588, Valid Loss: 0.26092946\n",
      "Starting epoch 5\n",
      "Epoch [5/10],Train Loss: 0.2559, Valid Loss: 0.26056942\n",
      "Starting epoch 6\n",
      "Epoch [6/10],Train Loss: 0.2519, Valid Loss: 0.26649896\n",
      "Starting epoch 7\n",
      "Epoch [7/10],Train Loss: 0.2572, Valid Loss: 0.25427294\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 8\n",
      "Epoch [8/10],Train Loss: 0.2549, Valid Loss: 0.25899915\n",
      "Starting epoch 9\n",
      "Epoch [9/10],Train Loss: 0.2548, Valid Loss: 0.25166901\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 10\n",
      "Epoch [10/10],Train Loss: 0.2525, Valid Loss: 0.25549522\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# actual training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.0006)\n",
    "num_epochs = 10\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "save_path = 'siameseNet-batchnorm50.pt'\n",
    "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97b4b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ccd925da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on previously saved models\n",
    "import torch.optim as optim\n",
    "load_model = Net().to(device)\n",
    "load_optimizer = optim.Adam(load_model.parameters(), lr=0.0006)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd772b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== siameseNet-batchnorm50.pt\n",
      "0.2516690056300026\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "eval_every = 1000\n",
    "total_step = len(train_loader)*num_epochs\n",
    "best_val_loss = load_checkpoint(load_model, load_optimizer)\n",
    "\n",
    "print(best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "41c8663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Iteration\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21032/3331424853.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21032/2237250499.py\u001b[0m in \u001b[0;36mevaluation\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting Iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmainAudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mmainAudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmainAudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mpredVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21032/3316115860.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mtestAudioDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtestCategory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0mtestAudioDir\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0maudioDir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                     \u001b[0mtestAudioDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtestCategory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[0mtestAudioName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestAudioDir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mtestAudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestAudioDir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestAudioName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluation(load_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c251f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d21954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"How to use the evaluation n way:\n",
    "\n",
    "# Set the parameters\n",
    "testSize = 5000 # how big you want your test size to be\n",
    "numWay = 4 # how many ways metric\n",
    "\n",
    "# Create the dataset for it and put it into dataloader\n",
    "test_set = NWayOneShotEvalSet(categories, root_dir, testSize, numWay, transformations) \n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, num_workers = 2)\n",
    "\n",
    "\n",
    "# Load the models (the name of the loaded model can be changed in the load_checkpoint() function)\n",
    "load_model = Net().to(device)\n",
    "load_optimizer = optim.Adam(load_model.parameters(), lr=0.0006)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "eval_every = 1000\n",
    "total_step = len(train_loader)*num_epochs\n",
    "best_val_loss = load_checkpoint(load_model, load_optimizer)\n",
    "\n",
    "print(best_val_loss)\n",
    "\n",
    "# Evaluate from the test loader \n",
    "\n",
    "eval(load_model, test_loader)\n",
    "\n",
    "\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d6a54d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5d3f5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12524/1965261682.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnvidia\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msmi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5306f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  1 15:14:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.41       Driver Version: 471.41       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0 Off |                  N/A |\n",
      "| N/A   50C    P8     8W /  N/A |   1101MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4884      C   ...\\envs\\torchgpu\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78094fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
